{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71355431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Txn Hash           Type   Block               From  \\\n",
      "0  0x7bf7d...e5f4b87  Coin_Transfer  485665  0x4E5FF...9CC3C31   \n",
      "1  0x4774f...a4fe408  Coin_Transfer  476179  0x400D4...d27cE1C   \n",
      "2  0xd6147...7613120  Coin_Transfer  469467  0x400D4...d27cE1C   \n",
      "3  0x08830...cbffce7  Coin_Transfer  463260  0x400D4...d27cE1C   \n",
      "4  0x75689...541cc33  Coin_Transfer  451243  0x400D4...d27cE1C   \n",
      "\n",
      "                  To                     Timestamp   Txn Fee  Value(CINT)  \\\n",
      "0  0x1f434...88444e0  2d ago\\nJul, 15, 2025, 03:03  0.000008          0.1   \n",
      "1  0xA48E1...a0D9630  2d ago\\nJul, 14, 2025, 11:11  0.000000         20.0   \n",
      "2  0x67694...b304ccf  3d ago\\nJul, 13, 2025, 23:57  0.000000         20.0   \n",
      "3  0x6bf85...1Aec29a  3d ago\\nJul, 13, 2025, 13:34  0.000000         20.0   \n",
      "4  0x75619...079EAB9  4d ago\\nJul, 12, 2025, 17:27  0.000000         20.0   \n",
      "\n",
      "    Status  \n",
      "0  Success  \n",
      "1  Success  \n",
      "2  Success  \n",
      "3  Success  \n",
      "4  Success  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = \"../WebScraping/Transactions.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9377c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Txn Hash', 'Type', 'Block', 'From', 'To', 'Timestamp', 'Txn Fee', 'Value(CINT)', 'Status']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3896095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txn_hash</th>\n",
       "      <th>type</th>\n",
       "      <th>block</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>txn_fee_cint</th>\n",
       "      <th>value_cint</th>\n",
       "      <th>status</th>\n",
       "      <th>direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x10927...b039478</td>\n",
       "      <td>Contract_Creation</td>\n",
       "      <td>100047</td>\n",
       "      <td>0xd08b8...17bd86c</td>\n",
       "      <td>0x5c2b1...8241879</td>\n",
       "      <td>2025-06-18 06:01:00</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Success</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x09cf8...b2bace6</td>\n",
       "      <td>Contract_Creation</td>\n",
       "      <td>100055</td>\n",
       "      <td>0xd08b8...17bd86c</td>\n",
       "      <td>0xda990...92b972f</td>\n",
       "      <td>2025-06-18 06:02:00</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Success</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xdae48...30a14d6</td>\n",
       "      <td>Contract_Creation</td>\n",
       "      <td>100055</td>\n",
       "      <td>0xd08b8...17bd86c</td>\n",
       "      <td>0xb81ea...61b7583</td>\n",
       "      <td>2025-06-18 06:02:00</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Success</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x3e78c...fbce879</td>\n",
       "      <td>Contract_Creation</td>\n",
       "      <td>100055</td>\n",
       "      <td>0xd08b8...17bd86c</td>\n",
       "      <td>0x1aa27...35302d1</td>\n",
       "      <td>2025-06-18 06:02:00</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Success</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xbcbdf...537d2e2</td>\n",
       "      <td>Contract_Creation</td>\n",
       "      <td>100055</td>\n",
       "      <td>0xd08b8...17bd86c</td>\n",
       "      <td>0x39351...87193a2</td>\n",
       "      <td>2025-06-18 06:02:00</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Success</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0xe670c...c90f019</td>\n",
       "      <td>Contract_Creation</td>\n",
       "      <td>100055</td>\n",
       "      <td>0xd08b8...17bd86c</td>\n",
       "      <td>0x1a6e3...5dccdf1</td>\n",
       "      <td>2025-06-18 06:02:00</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Success</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x4caa0...01d215f</td>\n",
       "      <td>Contract_Creation</td>\n",
       "      <td>100054</td>\n",
       "      <td>0xd08b8...17bd86c</td>\n",
       "      <td>0xc352f...8e61994</td>\n",
       "      <td>2025-06-18 06:02:00</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Success</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0xae111...046aef8</td>\n",
       "      <td>Contract_Creation</td>\n",
       "      <td>100054</td>\n",
       "      <td>0xd08b8...17bd86c</td>\n",
       "      <td>0x4c040...1c62d85</td>\n",
       "      <td>2025-06-18 06:02:00</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Success</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x933d8...3fe2293</td>\n",
       "      <td>Contract_Creation</td>\n",
       "      <td>100055</td>\n",
       "      <td>0xd08b8...17bd86c</td>\n",
       "      <td>0x7d769...12b7e26</td>\n",
       "      <td>2025-06-18 06:02:00</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Success</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x12fb5...2b558a7</td>\n",
       "      <td>Contract_Creation</td>\n",
       "      <td>100054</td>\n",
       "      <td>0xd08b8...17bd86c</td>\n",
       "      <td>0x46d47...83c90c6</td>\n",
       "      <td>2025-06-18 06:02:00</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Success</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            txn_hash               type   block               from  \\\n",
       "0  0x10927...b039478  Contract_Creation  100047  0xd08b8...17bd86c   \n",
       "1  0x09cf8...b2bace6  Contract_Creation  100055  0xd08b8...17bd86c   \n",
       "2  0xdae48...30a14d6  Contract_Creation  100055  0xd08b8...17bd86c   \n",
       "3  0x3e78c...fbce879  Contract_Creation  100055  0xd08b8...17bd86c   \n",
       "4  0xbcbdf...537d2e2  Contract_Creation  100055  0xd08b8...17bd86c   \n",
       "5  0xe670c...c90f019  Contract_Creation  100055  0xd08b8...17bd86c   \n",
       "6  0x4caa0...01d215f  Contract_Creation  100054  0xd08b8...17bd86c   \n",
       "7  0xae111...046aef8  Contract_Creation  100054  0xd08b8...17bd86c   \n",
       "8  0x933d8...3fe2293  Contract_Creation  100055  0xd08b8...17bd86c   \n",
       "9  0x12fb5...2b558a7  Contract_Creation  100054  0xd08b8...17bd86c   \n",
       "\n",
       "                  to           timestamp  txn_fee_cint  value_cint   status  \\\n",
       "0  0x5c2b1...8241879 2025-06-18 06:01:00      0.000008         0.0  Success   \n",
       "1  0xda990...92b972f 2025-06-18 06:02:00      0.000008         0.0  Success   \n",
       "2  0xb81ea...61b7583 2025-06-18 06:02:00      0.000008         0.0  Success   \n",
       "3  0x1aa27...35302d1 2025-06-18 06:02:00      0.000008         0.0  Success   \n",
       "4  0x39351...87193a2 2025-06-18 06:02:00      0.000008         0.0  Success   \n",
       "5  0x1a6e3...5dccdf1 2025-06-18 06:02:00      0.000008         0.0  Success   \n",
       "6  0xc352f...8e61994 2025-06-18 06:02:00      0.000008         0.0  Success   \n",
       "7  0x4c040...1c62d85 2025-06-18 06:02:00      0.000008         0.0  Success   \n",
       "8  0x7d769...12b7e26 2025-06-18 06:02:00      0.000008         0.0  Success   \n",
       "9  0x46d47...83c90c6 2025-06-18 06:02:00      0.000008         0.0  Success   \n",
       "\n",
       "  direction  \n",
       "0     other  \n",
       "1     other  \n",
       "2     other  \n",
       "3     other  \n",
       "4     other  \n",
       "5     other  \n",
       "6     other  \n",
       "7     other  \n",
       "8     other  \n",
       "9     other  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load CSV\n",
    "csv_path = \"../WebScraping/Transactions.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2. Drop duplicate Txn Hashes\n",
    "df = df.drop_duplicates(subset=[\"Txn Hash\"])\n",
    "\n",
    "# 3. Standardize & rename columns\n",
    "df = df.rename(columns=lambda c: (\n",
    "    c.strip()\n",
    "     .lower()\n",
    "     .replace(\" \", \"_\")\n",
    "     .replace(\"(\", \"\")\n",
    "     .replace(\")\", \"\")\n",
    "     .replace(\"-\", \"_\")\n",
    "))\n",
    "\n",
    "df = df.rename(columns={\n",
    "    \"valuecint\": \"value_cint\",    # rename the raw\n",
    "    \"txn_fee\":   \"txn_fee_cint\",\n",
    "})\n",
    "# now drop any leftover\n",
    "if \"valuecint\" in df.columns:\n",
    "    df = df.drop(columns=[\"valuecint\"])\n",
    "# e.g. \"Txn Hash\" → \"txn_hash\", \"Txn Fee (CINT)\" → \"txn_fee_cint\"\n",
    "\n",
    "# 4. Parse and normalize timestamps\n",
    "df[\"timestamp_clean\"] = (\n",
    "    df[\"timestamp\"]\n",
    "      .astype(str)\n",
    "      .str.split(\"\\n\")\n",
    "      .str[-1]                # take the 'Jul, 15, 2025, 03:03' portion\n",
    ")\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp_clean\"], format=\"%b, %d, %Y, %H:%M\", errors=\"coerce\")\n",
    "df = df.drop(columns=\"timestamp_clean\")\n",
    "\n",
    "# 5. Normalize address fields\n",
    "for col in [\"from\", \"to\", \"contract_address\"]:\n",
    "    if col in df:\n",
    "        df[col] = df[col].str.lower().str.strip().replace(\"\", np.nan)\n",
    "\n",
    "# 6. Convert all on-chain numeric columns to floats/ints\n",
    "df[\"block\"]        = df[\"block\"].astype(int)\n",
    "df[\"value_cint\"]   = df[\"value_cint\"].astype(float)\n",
    "df[\"txn_fee_cint\"] = df[\"txn_fee_cint\"].astype(float)\n",
    "\n",
    "\n",
    "# 7. Derive domain-specific fields\n",
    "\n",
    "# 7b. Classify txn direction for a given “our” address set\n",
    "#    (you can supply your own key-addresses list)\n",
    "our_addrs = {\"0xabc…\", \"0xdef…\"}  \n",
    "def direction(row):\n",
    "    if row[\"from\"] in our_addrs and row[\"to\"] not in our_addrs:\n",
    "        return \"outgoing\"\n",
    "    elif row[\"to\"] in our_addrs and row[\"from\"] not in our_addrs:\n",
    "        return \"incoming\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "df[\"direction\"] = df.apply(direction, axis=1)\n",
    "\n",
    "# 8. Drop or archive any truly irrelevant fields\n",
    "#    e.g., if ‘status’ is always ‘Success’, drop it; if you’ll never need raw input_data, drop that.\n",
    "if \"status\" in df and df[\"status\"].nunique() == 1:\n",
    "    df = df.drop(columns=[\"status\"])\n",
    "for col in [\"input_data\", \"logs\"] :\n",
    "    if col in df:\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "# 9. Sort & re-index\n",
    "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# 10. Final integrity checks\n",
    "assert df[\"txn_hash\"].is_unique, \"Txn Hash is not unique!\"\n",
    "assert df[[\"block\", \"timestamp\"]].notnull().all().all(), \"Missing block or timestamp!\"\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894a59ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface-hub==0.15.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\datta\\anaconda3\\lib\\site-packages (1.23.5)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/60.9 kB 660.6 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 51.2/60.9 kB 890.4 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.9/60.9 kB 405.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\datta\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\datta\\anaconda3\\lib\\site-packages (from huggingface-hub==0.15.1) (3.13.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\datta\\anaconda3\\lib\\site-packages (from huggingface-hub==0.15.1) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\datta\\anaconda3\\lib\\site-packages (from huggingface-hub==0.15.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\datta\\anaconda3\\lib\\site-packages (from huggingface-hub==0.15.1) (4.67.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\datta\\anaconda3\\lib\\site-packages (from huggingface-hub==0.15.1) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\datta\\anaconda3\\lib\\site-packages (from huggingface-hub==0.15.1) (4.14.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\datta\\anaconda3\\lib\\site-packages (from huggingface-hub==0.15.1) (23.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\datta\\anaconda3\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\datta\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\datta\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.6-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\datta\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub==0.15.1) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\datta\\anaconda3\\lib\\site-packages (from requests->huggingface-hub==0.15.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\datta\\anaconda3\\lib\\site-packages (from requests->huggingface-hub==0.15.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\datta\\anaconda3\\lib\\site-packages (from requests->huggingface-hub==0.15.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\datta\\anaconda3\\lib\\site-packages (from requests->huggingface-hub==0.15.1) (2025.7.14)\n",
      "Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "   ---------------------------------------- 0.0/236.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 236.8/236.8 kB 14.2 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.7.0-cp311-cp311-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.1/10.7 MB 44.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.1/10.7 MB 54.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.4/10.7 MB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 65.1 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.6-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 3.7/12.9 MB 78.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.7/12.9 MB 82.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.5/12.9 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 59.4 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, huggingface-hub, scikit-learn\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.4\n",
      "    Uninstalling huggingface-hub-0.33.4:\n",
      "      Successfully uninstalled huggingface-hub-0.33.4\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.5.2\n",
      "    Uninstalling scikit-learn-1.5.2:\n",
      "      Successfully uninstalled scikit-learn-1.5.2\n",
      "Successfully installed huggingface-hub-0.15.1 numpy-2.2.6 scikit-learn-1.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\datta\\anaconda3\\Lib\\site-packages\\numpy\\~-ibs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\datta\\AppData\\Local\\Temp\\pip-uninstall-jj8n138e'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\datta\\anaconda3\\Lib\\site-packages\\~-learn'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "accelerate 1.1.1 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.2.6 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.\n",
      "datasets 3.1.0 requires huggingface-hub>=0.23.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.59.0 requires numpy<1.27,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "peft 0.13.2 requires huggingface-hub>=0.17.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.2.6 which is incompatible.\n",
      "sentence-transformers 5.0.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "streamlit 1.30.0 requires numpy<2,>=1.19.3, but you have numpy 2.2.6 which is incompatible.\n",
      "streamlit 1.30.0 requires protobuf<5,>=3.20, but you have protobuf 6.31.1 which is incompatible.\n",
      "streamlit 1.30.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "tensorflow-intel 2.12.0rc0 requires numpy<1.24,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow-intel 2.12.0rc0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 2.2.6 which is incompatible.\n",
      "tokenizers 0.20.3 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "torchvision 0.19.0 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n",
      "transformers 4.46.3 requires huggingface-hub<1.0,>=0.23.2, but you have huggingface-hub 0.15.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade huggingface-hub==0.15.1 numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a20e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing huggingface_hub.inference._client: No module named 'huggingface_hub.inference'\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\datta\\AppData\\Local\\Temp\\ipykernel_41452\\3058143426.py\", line 10, in <module>\n",
      "    from huggingface_hub import InferenceClient\n",
      "  File \"c:\\Users\\datta\\anaconda3\\Lib\\site-packages\\huggingface_hub\\__init__.py\", line 998, in __getattr__\n",
      "  File \"c:\\Users\\datta\\anaconda3\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'huggingface_hub.inference'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\datta\\AppData\\Roaming\\Python\\Python311\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import InferenceClient\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Set your HF token (you should already have this in your MCP-VSCode env):\n",
    "#    export HUGGINGFACEHUB_API_TOKEN=\"hf_xxx…\"\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "HF_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "client   = InferenceClient(token=HF_TOKEN)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Load your preprocessed DataFrame (must already have run your cleaning code):\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# e.g. if you saved it:\n",
    "# df = pd.read_csv(\"../WebScraping/Transactions_cleaned.csv\", parse_dates=[\"timestamp\"])\n",
    "# Here, assume df is already in memory:\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Turn each row into a human‐readable “doc” string\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def row_to_doc(r):\n",
    "    return (\n",
    "        f\"Txn {r.txn_hash}: {r.type} of {r.value_cint} CINT on \"\n",
    "        f\"{r.timestamp.strftime('%Y-%m-%d %H:%M')} \"\n",
    "        f\"from {r.from_address} to {r.to_address}, fee={r.txn_fee_cint}\"\n",
    "    )\n",
    "\n",
    "docs = df.apply(row_to_doc, axis=1).tolist()\n",
    "ids  = df[\"txn_hash\"].tolist()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Embed all docs with HF’s embeddings endpoint\n",
    "#    (this calls the all-MiniLM core model hosted via your MCP servers)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Note: this may take a minute for large dfs — it’s batching internally.\n",
    "embeddings = client.feature_extraction(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    inputs=docs\n",
    ")\n",
    "# embeddings is a List[List[float]] of shape (len(docs), D)\n",
    "emb = np.array(embeddings)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5) At query time: embed the user’s question, retrieve top-k, then generate\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def answer_query(query: str, top_k: int = 5):\n",
    "    # 5a) Embed the query\n",
    "    q_emb = np.array(client.feature_extraction(\n",
    "        model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        inputs=[query]\n",
    "    )[0]).reshape(1, -1)\n",
    "\n",
    "    # 5b) Find top_k similar docs by cosine similarity\n",
    "    sims      = cosine_similarity(q_emb, emb)[0]\n",
    "    best_idx  = sims.argsort()[-top_k:][::-1]\n",
    "    context   = \"\\n\".join(docs[i] for i in best_idx)\n",
    "\n",
    "    # 5c) Assemble the prompt\n",
    "    prompt = (\n",
    "        \"You are a blockchain analytics assistant. \"\n",
    "        \"Here are some transactions that may be relevant:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"User question: {query}\\n\"\n",
    "        \"Answer concisely:\"\n",
    "    )\n",
    "\n",
    "    # 5d) Call the HF text-generation endpoint\n",
    "    output = client.text_generation(\n",
    "        model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "        inputs=prompt,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False\n",
    "    )\n",
    "    return output.generated_text\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6) Try it out\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(answer_query(\"Show me the three largest outgoing transactions in the last week.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aca25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
